<!DOCTYPE html>
<html lang="en">


<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="styles.css">
    <title>ConsistencyTTA</title>
</head>


<body>
    <script src="js_script.js"></script>

    <header>
        <h1><span style="font-size: 40px;">ConsistencyTTA: </span>
            <span style="font-weight: 500; font-size: 28px;">Accelerating Diffusion-Based<br>
                Text-to-Audio Generation with Consistency Distillation</span>
        </h1>

        <h3 style="font-weight: 400;">
            <a class="title_link" href="https://bai-yt.github.io"
                target=&ldquo;blank&rdquo;><u>Yatong Bai</u></a>,
            <a class="title_link" href="https://www.microsoft.com/applied-sciences/people/trung-dang"
                target=&ldquo;blank&rdquo;><u>Trung Dang</u></a>,
            <a class="title_link" href="https://www.microsoft.com/applied-sciences/people/dung-tran"
                target=&ldquo;blank&rdquo;><u>Dung Tran</u></a>,
            <a class="title_link" href="https://www.microsoft.com/applied-sciences/people/kazuhito-koishida"
                target=&ldquo;blank&rdquo;><u>Kazuhito Koishida</u></a>,
            <a class="title_link" href="https://people.eecs.berkeley.edu/~sojoudi"
                target=&ldquo;blank&rdquo;><u>Somayeh Sojoudi</u></a>
        </h3>
        <h4 class="institution">
            <a style="color: #DDD; text-decoration: none" href="https://www.microsoft.com/applied-sciences" target=&blank;>
                Microsoft Applied Science Group</a>, &nbsp;&nbsp;&nbsp;&nbsp;
            <a style="color: #DDD; text-decoration: none" href="https://me.berkeley.edu/" target=&blank;>
                University of California, Berkeley</a> 
        </h4>
        <h4 class="venue">
            In <a class="venue_link" href="https://interspeech2024.org" target="_blank"><i>INTERSPEECH</i> 2024</a>
        </h4>

        <!-- Buttons -->
        <a href="demo.html"><button class="demo-button">Generation Examples</button></a>
        <a href="https://arxiv.org/abs/2309.10740" target="_blank">
            <button class="paper-button"><i class="ai ai-arxiv"></i>Paper</button>
        </a>
        <a href="https://huggingface.co/spaces/Bai-YT/ConsistencyTTA" target="_blank">
            <button class="livedemo-button">ðŸ¤— Live Demo</button>
        </a>
        <a href="https://github.com/Bai-YT/ConsistencyTTA" target="_blank">
            <button class="code-button"><i class="fab fa-github"></i>Code&nbsp;&nbsp;</button>
        </a>
        <a href="https://huggingface.co/Bai-YT/ConsistencyTTA" target="_blank">
            <button class="hf-button">Model Checkpoints</button>
        </a>

        <div style="height: 7px;"></div>
    </header>

    <section class="section">
        <h2>Description</h2>
        <p>
            Diffusion models power a vast majority of the text-to-audio generation methods.
            Unfortunately, diffusion models suffer from a slow inference speed due to iteratively querying the
            underlying denoising network, thus unsuitable for applications with time or computational constraints.
            This work proposes text-to-audio models that only require <b><i>a single non-autoregressive neural network
            query</i></b>, accelerating the generation hundreds of times and enabling on-device audio generation.
        </p>
        <p>
            To achieve this, we propose "CFG-aware latent consistency model'', which moves consistency generation into
            a latent space and incorporates classifier-free guidance (CFG) into the training process.
            By doing so, our models retain diffusion models' impressive generation quality and diversity.
            Unlike diffusion models, ConsistencyTTA's single-step generation makes its generated audio available during training.
            We leverage this advantage to finetune ConsistencyTTA end-to-end with audio-space text-aware metrics,
            such as the CLAP score, further enhancing the generations.
            We use the CLAP loss as an example, confirming that end-to-end fine-tuning further boosts the generation quality.
        </p>
        <p>
            <b>Please check out <a href="poster.pdf" target="_blank">our poster</a> at 
                <a href="https://interspeech2024.org" target="_blank">INTERSPEECH 2024</a> at Kos Island, Greece!</b>
        </p>
    </section>

    <section class="section">
        <h2>Main Experiment Results</h2>
        <div class="center-container">
            <div class="content">
                <img src="main_figure_.png" alt="ConsistencyTTA Results" title="Results" width="450"/>
                &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                <video width="216" height="384" controls>
                    <source src="demo_video.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
            </div>
        </div>
        <p>
            Our method reduce the computation of the core step of diffusion-based text-to-audio generation by
            a factor of 400 and enables on-device generation, while observing minimal performance degradation in
            FrÃ©chet Audio Distance (FAD), FrÃ©chet Distance (FD), KL Divergence, and CLAP Scores.<br>
            Generation Time is the time in minutes to generate the entire validation set (882 samples).<br>
            <i>â†‘: higher is better; â†“: lower is better.</i>
        </p>
        <table class="result-table">
            <thead>
                <tr class="result-row">
                    <th class="result-head"></th>
                    <th class="result-head-2">Model Queries<br>â†“</th> <th class="result-head-2">Generation Time<br>â†“</th>
                    <th class="result-head">Subjective Quality<br>â†‘</th> <th class="result-head">Subjective Text Align<br>â†‘</th>
                    <th class="result-head-2">CLAP<sub>T</sub><br>â†‘</th> <th class="result-head-2">CLAP<sub>A</sub><br>â†‘</th>
                    <th class="result-head-2">FAD<br>â†“</th> <th class="result-head-2">FD<br>â†“</th> <th class="result-head-2">KLD<br>â†“</th>
                </tr>
            </thead>
            <tbody>
                <tr class="result-row-2" style="color: #898989">
                    <td class="result-data-small">AudioLDM-L (Baseline)</td> <td class="result-data-2">400</td>
                    <td class="result-data-2">-</td> <td class="result-data">-</td>
                    <td class="result-data">-</td> <td class="result-data-2">-</td> <td class="result-data-2">-</td>
                    <td class="result-data-2-400">2.08</td> <td class="result-data-2">27.12</td> <td class="result-data-2">1.86</td>
                </tr>
                <tr class="result-row-2" style="color: #898989">
                    <td class="result-data-small">TANGO (Baseline)</td>
                    <td class="result-data-2">400</td> <td class="result-data-2">168</td>
                    <td class="result-data"><b>4.136</b></td> <td class="result-data"><b>4.064</b></td>
                    <td class="result-data-2-400">24.10</td> <td class="result-data-2"><b>72.85</b></td>
                    <td class="result-data-2"><b>1.631</b></td> <td class="result-data-2"><b>20.11</b></td> <td class="result-data-2">1.362</td>
                </tr>
                <tr class="result-row">
                    <td class="result-data-small">ConsistencyTTA + CLAP-FT</td>
                    <td class="result-data-2"><b>1</b></td> <td class="result-data-2"><b>2.3</b></td>
                    <td class="result-data">3.830</td> <td class="result-data"><b>4.064</b></td>
                    <td class="result-data-2"><b>24.69</b></td> <td class="result-data-2-400">72.54</td>
                    <td class="result-data-2">2.406</td> <td class="result-data-2-400">20.97</td> <td class="result-data-2-400">1.358</td>
                </tr>
                <tr class="result-row">
                    <td class="result-data-small">ConsistencyTTA</td>
                    <td class="result-data-2"><b>1</b></td> <td class="result-data-2"><b>2.3</b></td>
                    <td class="result-data-400">3.902</td> <td class="result-data">4.010</td>
                    <td class="result-data-2">22.50</td> <td class="result-data-2">72.30</td>
                    <td class="result-data-2">2.575</td> <td class="result-data-2">22.08</td>
                    <td class="result-data-2"><b>1.354</b></td>
                </tr>
                <tr class="result-row-2-small" style="color: #898989">
                    <td class="result-data-small">Ground Truth</td> <td class="result-data-2">-</td>
                    <td class="result-data-2">-</td> <td class="result-data">-</td> <td class="result-data">-</td>
                    <td class="result-data-2">26.71</td> <td class="result-data-2">100</td>
                    <td class="result-data-2">-</td> <td class="result-data-2">-</td> <td class="result-data-2">-</td>
                </tr>
            </tbody>
        </table>
        <p>
            <a href="https://paperswithcode.com/sota/audio-generation-on-audiocaps" target=&ldquo;blank&rdquo;>This benchmark</a>
            demonstrates how our single-step models stack up with previous methods,
            most of which requiring hundreds of generation steps.
        </p>
    </section>

    <section class="section">
        <h2>Ablation Studies on Distillation Settings</h2>
        <p>
            <table class="result-table">
                <thead>
                    <tr class="result-row">
                        <th class="result-head">Guidance Method</th>
                        <th class="result-head">CFG Weight</th>
                        <th class="result-head">Teacher Solver</th>
                        <th class="result-head">Noise Schedule</th>
                        <th class="result-head-2">FAD â†“</th>
                        <th class="result-head-2">FD â†“</th>
                        <th class="result-head-2">KLD â†“</th>
                    </tr>
                </thead>
                <tbody>
                    <tr class="result-row-2">
                        <td class="result-data-small">Unguided</td>
                        <td class="result-data-small">1</td>
                        <td class="result-data-small">DDIM</td>
                        <td class="result-data-small">Uniform</td>
                        <td class="result-data-2">13.48</td>
                        <td class="result-data-2">45.75</td>
                        <td class="result-data-2">2.409</td>
                    </tr>
                    <tr class="result-row-2">
                        <td class="result-data-small" rowspan="2">External CFG</td>
                        <td class="result-data-small" rowspan="2">3</td>
                        <td class="result-data-small">DDIM</td>
                        <td class="result-data-small">Uniform</td>
                        <td class="result-data-2">8.565</td>
                        <td class="result-data-2">38.67</td>
                        <td class="result-data-2">2.015</td>
                    </tr>
                    <tr class="result-row-2">
                        <td class="result-data-small">Heun</td>
                        <td class="result-data-small">Karras</td>
                        <td class="result-data-2">7.421</td>
                        <td class="result-data-2">39.36</td>
                        <td class="result-data-2">1.976</td>
                    </tr>
                    <tr class="result-row-2">
                        <td class="result-data-small" rowspan="2">CFG Distillation<br>with Fixed Weight</td>
                        <td class="result-data-small" rowspan="2">3</td>
                        <td class="result-data-small" rowspan="2">Heun</td>
                        <td class="result-data-small">Karras</td>
                        <td class="result-data-2">5.702</td>
                        <td class="result-data-2">33.18</td>
                        <td class="result-data-2">1.494</td>
                    </tr>
                    <tr class="result-row-2">
                        <td class="result-data-small">Uniform</td>
                        <td class="result-data-2">3.859</td>
                        <td class="result-data-2"><b>27.79</b></td>
                        <td class="result-data-2">1.421</td>
                    </tr>
                    <tr class="result-row-2">
                        <td class="result-data-small" rowspan="3">CFG Distillation<br>with Random Weight</td>
                        <td class="result-data-small">4</td>
                        <td class="result-data-small" rowspan="2">Heun</td>
                        <td class="result-data-small" rowspan="2">Uniform</td>
                        <td class="result-data-2-400">3.180</td>
                        <td class="result-data-2-400">27.92</td>
                        <td class="result-data-2-400">1.394</td>
                    </tr>
                    <tr class="result-row-2">
                        <td class="result-data-small">6</td>
                        <td class="result-data-2"><b>2.975</b></td>
                        <td class="result-data-2">28.63</td>
                        <td class="result-data-2"><b>1.378</b></td>
                    </tr>
                </tbody>
            </table>            
            Based on these results, we can conclude that:
            <ul>
                <li>CFG distillation with random weight is more effective than fixed weight, 
                    which is more effective than external CFG.</li>
                <li>Heun is a better teacher solver than DDIM, and 
                    Uniform noise schedule outperforms Karras noise schedule.</li>
            </ul>         
        </p>
    </section>

    <section class="section">
        <h2>Generation Diversity</h2>
        <p>
            Consistency models demonstrate non-trivial generation diversity, as do diffusion models.
            In <a href="diversity.html">this page</a>, we present 50 groups of generations from
            four different random seeds to demonstrate this diversity, showing that our method
            combines the diversity of diffusion models and the efficiency of single-step models.
        </p>
    </section>

    <section class="section">
        <h2>Human Evaluation</h2>
        <p>
            ConsistencyTTA's performance is verified via extensive human evaluation.
            Audio clips generated from ConsistencyTTA and baseline methods are mixed and shown to the evaluators,
            who are then asked to rate the audio clips based on their quality and correspondence with the textual prompt.
            A sample of the evaluation form is shown on <a href="evaluation.html">this page</a>.
        </p>
    </section>

    <section class="section">
        <h2>Citing Our Work (BibTeX)</h2>
        <div id="bibtex1" class="bibtex" onclick="copyToClipboard('bibtex1')">
            <i class="far fa-copy copy-icon"></i>
<pre>@inproceedings{bai2024consistencytta,
  author = {Bai, Yatong and Dang, Trung and Tran, Dung and Koishida, Kazuhito and Sojoudi, Somayeh},
  title = {ConsistencyTTA: Accelerating Diffusion-Based Text-to-Audio Generation with Consistency Distillation},
  booktitle = {INTERSPEECH},
  year = {2024}
}</pre>
        </div>
    </section>

    <section class="section">
        <h2>Contact</h2>
        <p>
            For any questions or suggestions regarding our work, please email
            <a href="mailto:yatong_bai@berkeley.edu">yatong_bai@berkeley.edu</a> and
            <a href="mailto:dung.tran@microsoft.com">dung.tran@microsoft.com</a>.
        </p>
    </section>

    <footer>
        <p>&copy; Microsoft and UC Berkeley. All rights reserved.</p>
    </footer>
</body>


</html>
