<!DOCTYPE html>
<html lang="en">


<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="styles.css">
    <title>Consistency TTA Generation</title>
</head>


<body>
    <script src="js_script.js"></script>

    <header>
        <h1>Accelerating Diffusion-Based Text-to-Audio<br>Generation with Consistency Distillation</h1>

        <h3 style="font-weight: 400;">
            <a class="title_link" href="https://bai-yt.github.io" target=&ldquo;blank&rdquo;><u>Yatong Bai</u></a>, 
            Trung Dang, Dung Tran, Kazuhito Koishida, Somayeh Sojoudi
        </h3>
        <h4 class="institution">
            <a style="color: #DDD; text-decoration: none" href="https://www.microsoft.com/applied-sciences" target=&blank;>
                Microsoft Applied Science Group</a>, &nbsp;&nbsp;&nbsp;&nbsp;
            <a style="color: #DDD; text-decoration: none" href="https://me.berkeley.edu/" target=&blank;>
                University of California, Berkeley</a>
        </h4>
        <div style="height: 13px;"></div>

        <!-- Buttons -->
        <a href="demo.html"><button class="demo-button">Demo page</button></a>
        <a href="https://arxiv.org/abs/2309.10740" target="_blank">
          <button class="paper-button">Preprint paper</button>
        </a>
        <a href="https://github.com/Consistency-TTA/consistency-tta.github.io" target="_blank">
            <i class="fab fa-github"></i>
        </a>        
        <a href="evaluation.html"><button class="eval-button">Sample evaluation form</button></a>

        <div style="height: 7px;"></div>
    </header>

    <section class="section">
        <h2>Description</h2>
        <p>
            Diffusion models power a vast majority of the text-to-audio generation methods.
            Unfortunately, diffusion models suffer from a slow inference speed due to iteratively querying the
            underlying denoising network, thus unsuitable for applications with time or computational constraints.
            This work modifies the recently proposed "consistency distillation" framework to train text-to-audio 
            models that only require a single neural network query, accelerating the generation hundreds of times.
        </p>
        <p>
            By incorporating classifier-free guidance into the distillation framework, our models retain
            diffusion models' impressive generation quality and diversity. Furthermore, the non-recurrent
            differentiable structure resulting from the distillation allows fine-tuning with novel loss functions.
            We use the CLAP loss as an example, confirming that end-to-end fine-tuning 
            further boosts the generation quality.
        </p>
    </section>

    <section class="section">
        <h2>Main Experiment Results</h2>
        <p>
            Our method reduce the computation of the core step of diffusion-based text-to-audio generation by 
            a factor of 400, while observing minimal performance degradation in terms of 
            Fréchet Audio Distance (FAD), Fréchet Distance (FD), KL Divergence, and CLAP Scores.
        </p>
        <table class="result-table">
            <thead>
                <tr class="result-row">
                    <th class="result-head"></th> <th class="result-head"># queries (↓)</th>
                    <th class="result-head">CLAP<sub>T</sub> (↑)</th> <th class="result-head">CLAP<sub>A</sub> (↑)</th>
                    <th class="result-head">FAD (↓)</th> <th class="result-head">FD (↓)</th> <th class="result-head">KLD (↓)</th>
                </tr>
            </thead>
            <tbody>
                <tr class="result-row" style="color: #a0a0a0">
                    <td class="result-data">Diffusion (Baseline)</td> <td class="result-data">400</td>
                    <td class="result-data">24.57</td> <td class="result-data">72.79</td>
                    <td class="result-data">1.908</td> <td class="result-data">19.57</td> <td class="result-data">1.350</td>
                </tr>
                <tr class="result-row">
                    <td class="result-data">Consistency + CLAP FT (Ours)</td> <td class="result-data">1</td>
                    <td class="result-data">24.69</td> <td class="result-data">72.54</td>
                    <td class="result-data">2.406</td> <td class="result-data">20.97</td> <td class="result-data">1.358</td>
                </tr>
                <tr class="result-row">
                    <td class="result-data">Consistency (Ours)</td> <td class="result-data">1</td>
                    <td class="result-data">22.50</td> <td class="result-data">72.30</td>
                    <td class="result-data">2.575</td> <td class="result-data">22.08</td> <td class="result-data">1.354</td>
                </tr>
            </tbody>
        </table>
        <p>
            <a href="https://paperswithcode.com/sota/audio-generation-on-audiocaps" target=&ldquo;blank&rdquo;>This benchmark</a>
            demonstrates how our single-step models stack up with previous methods,
            most of which mostly require hundreds of generation steps.
        </p>
    </section>

    <section class="section">
        <h2>Generation Diversity</h2>
        <p>
            Consistency models demonstrate non-trivial generation diversity, as do diffusion models.
            In <a href="diversity.html">this page</a>, we present 50 groups of generations from
            four different random seeds to demonstrate this diversity, showing that our method
            combines the diversity of diffusion models and the efficiency of single-step models.
        </p>
    </section>

    <section class="section">
        <h2>BibTeX</h2>
        <div id="bibtex1" class="bibtex" onclick="copyToClipboard('bibtex1')">
            <i class="far fa-copy copy-icon"></i>
<pre>@article{bai2023accelerating,
  author = {Bai, Yatong and Dang, Trung and Tran, Dung and Koishida, Kazuhito and Sojoudi, Somayeh},
  title = {Accelerating Diffusion-Based Text-to-Audio Generation with Consistency Distillation},
  journal={arXiv preprint arXiv:2309.10740},
  year = {2023}
}</pre>
        </div>
    </section>

    <section class="section">
        <h2>Contact</h2>
        <p>
            For any questions regarding our work, please email
            <a href="mailto:yatong_bai@berkeley.edu">yatong_bai@berkeley.edu</a>.
            We are more than happy to help with implenting our method and verifying our results.
        </p>
    </section>

    <footer>
        <p>&copy; Microsoft and UC Berkeley. All rights reserved.</p>
    </footer>
</body>


</html>
