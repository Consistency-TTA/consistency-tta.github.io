<!DOCTYPE html>
<html lang="en">


<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="styles.css">
    <title>Consistency TTA Generation</title>
</head>


<body>
    <script src="js_script.js"></script>

    <header>
        <h1>Accelerating Diffusion-Based Text-to-Audio<br>Generation with Consistency Distillation</h1>
        <h3><a href="https://bai-yt.github.io" target=&ldquo;blank&rdquo;><u>Yatong Bai</u></a>, 
            Trung Dang, Dung Tran, Kazuhito Koishida, Somayeh Sojoudi
        </h3>

        <div style="height:13px;"></div>

        <!-- Buttons -->
        <a href="demo.html"><button class="demo-button">Demo page</button></a>
        <a href="https://arxiv.org/abs/2309.10740" target="_blank">
          <button class="paper-button">Preprint paper</button>
        </a>
        <a href="https://github.com/Consistency-TTA/consistency-tta.github.io" target="_blank">
            <i class="fab fa-github"></i>
        </a>        
        <a href="evaluation.html"><button class="eval-button">Sample evaluation form</button></a>

        <div style="height:7px;"></div>
    </header>

    <section class="section">
        <h2>Description</h2>
        <p>
            Diffusion models power a vast majority of the text-to-audio generation methods.
            Unfortunately, diffusion models suffer from a slow inference speed due to iteratively querying the
            underlying denoising network, thus unsuitable for applications with time or computational constraints.
            This work modifies the recently proposed "consistency distillation" framework to train text-to-audio 
            models that only require a single neural network query, accelerating the generation hundreds of times.
        </p>
        <p>
            By incorporating classifier-free guidance into the distillation framework, our models retain
            diffusion models' impressive generation quality and diversity. Furthermore, the non-recurrent
            differentiable structure resulting from the distillation allows fine-tuning with novel loss functions.
            We use the CLAP loss as an example, confirming that end-to-end fine-tuning 
            further boosts the generation quality.
        </p>
    </section>

    <section class="section">
        <h2>BibTeX</h2>
        <div id="bibtex1" class="bibtex" onclick="copyToClipboard('bibtex1')">
            <i class="far fa-copy copy-icon"></i>
<pre>@article{bai2023accelerating,
  author = {Yatong Bai, Trung Dang, Dung Tran, Kazuhito Koishida, Somayeh Sojoudi},
  title = {Accelerating Diffusion-Based Text-to-Audio Generation with Consistency Distillation},
  journal={arXiv preprint arXiv:2309.10740},
  year = {2023}
}</pre>
        </div>
    </section>

    <section class="section">
        <h2>Contact</h2>
        <p>
            For any questions regarding our work, please email
            <a href="mailto:yatong_bai@berkeley.edu">yatong_bai@berkeley.edu</a>.
            We are more than happy to help with implenting our method and verifying our results.
        </p>
    </section>

    <footer>
        <p>&copy; Microsoft and UC Berkeley. All rights reserved.</p>
    </footer>
</body>


</html>
